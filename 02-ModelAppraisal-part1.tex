\documentclass[PredictiveAnalytics101.tex]{subfiles} 
\begin{document} 
% PLAN FOR SECTION
% ------------------------------------
% Confusion Matrix
% Accuracy Precision Recall
% F-measures
% Cost of MisClassification
% True Positive Rate
% Specificity and Sensitivitiy

% Re-writing a section from a shitty PACKT book
%================================================ %
\begin{frame}
	\frametitle{The Coefficient of Determination}
	\Large
	\begin{itemize}
		\item The coefficient of determination $R^2$ is used in the context of statistical models whose main purpose is the prediction of future outcomes on the basis of other related information. 
		\item It is the proportion of variability in a data set that is accounted for by the statistical model. 
		\item It provides a measure of how well future outcomes are likely to be predicted by the model.
	\end{itemize}
\end{frame}
%================================================ %

\begin{frame}
	\frametitle{R-Squared}
	\Large
	\begin{itemize}
		\item $R^2$ is a statistic that will give some information about the goodness of fit of a model.
		\item In regression, the $R^2$ coefficient of determination is a statistical measure of how well the regression line approximates the real data points.
		\item An $R^2$ of 1.0 indicates that the regression line perfectly fits the data.
		
		\item In the case of simple linear regression, the coefficient of determination is equivalent to the squared value of the Pearson correlation coefficient. (Consider this to be co-incidental, rather than a definition)
	\end{itemize}
	
\end{frame}
%================================================ %
\begin{frame}
	\frametitle{The Adjusted Coefficient of Determination}
	\Large
	\begin{itemize}
		\item Adjusted $R^2$ (often written as and pronounced "\textbf{R bar squared}") is a modification of $R^2$ that adjusts for the number of predictor terms in a model.
		\item  Adjusted $R^2$ is used to compensate for the addition of variables to the model. 
		\item As more independent variables are added to the regression model, unadjusted $R^2$ will generally increase but there will never be a decrease.  
		\item This will occur even when the additional variables do little to help explain the dependent variable.
	\end{itemize}  
\end{frame}
%================================================ %

\begin{frame}
	\Large
	\begin{itemize}
		\item To compensate for this, adjusted $R^2$  is corrected for the number of independent variables in the model, increases only if the new term improves the model more than would be expected by chance. 
		\item If too many predictor variables are being used, this will be reflected in a reduced adjusted $R^2$. 
		\item The adjusted $R^2$ can be negative, and will always be less than or equal to $R^2$. 
		\item The result is an adjusted $R^2$ than can go up or down depending on whether the addition of another variable adds or does not add to the explanatory power of the model. \item Adjusted $R^2$ will always be lower than unadjusted.
	\end{itemize}
\end{frame}


%================================================ %

\begin{frame}
	\Large
	\begin{itemize}
		\item Adjusted R square is generally considered to be a more accurate goodness-of-fit measure than R square. 
		\item It has become standard practice to report the adjusted $R^2$, especially when there are multiple models presented with varying numbers of independent variables.
	\end{itemize}
\end{frame}
%
%$R^2_i$ is the unadjusted $R^2$ when you regress $X_i$ against all the other explanatory variables in the model.
%
%Suppose we have four independent variables $X_1$, $X_2$,$X_3$ and $X_4$.
%The unadjusted $R^2$ for the second variables $R^2_2$ is computed from the following linear model:
%
%\[ X_2 = \alpha + \beta_a X_1 + \beta_b X_3 + \beta_c X_4 \]
%
%N.B. The dependent variable $Y$ is not used at all in this case.
%
%
%Suppose there is no linear relation between $X_i$  and the other explanatory
%variables in the model. Then $R^2$ will be zero.
%The Variance inflation factor for slope estimate $\hat{\beta}_i$ is $ 1 / 1 - R^2_i $.
%
%The Tolerance for slope estimate $\hat{\beta}_i$ is $1 - R^2_i $.


%============================%
\begin{frame}
\frametitle{Confusion Matrix}

	\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{images/confusionmatrix2}
\end{figure}

\end{frame}

%==================================%
\begin{frame}
\frametitle{Accuracy}
\textbf{Accuracy}\\
Accuracy measures a fraction of the classifier's predictions that are correct.
{\LARGE
\[ \mbox{Accuracy} =  \frac{TN + TP }{TN+FN+TP+FP}  \]
}
\end{frame}
%========================================== %
\begin{frame}
\Large
\frametitle{Accuracy}
\begin{center}	
	\begin{tabular}{|c|c|c|}
		\hline  &  Predict P & Predict N  \\ 
		\hline True P  & 100 & 70  \\ 
		\hline True N & 30 & 9800 \\ 
		\hline 
	\end{tabular} 
\end{center}
\bigskip
{\LARGE
	\[ \mbox{Accuracy} =  \frac{9900 }{10,000} = 0.99  \]
}
\end{frame}
%==================================%
\begin{frame}
\Large
	\textbf{Accuracy}\\
\begin{itemize}
\item However, accuracy is not an informative metric if the proportions of
the classes are skewed in the population. (\textbf{Class Imbalance})
\item For example, a classifier that predicts
whether or not credit card transactions are fraudulent may be more sensitive to
false negatives than to false positives. 
\item To promote customer satisfaction, the credit
card company may prefer to risk verifying legitimate transactions than risk ignoring
a fraudulent transaction. 
\end{itemize}

\end{frame}
%==================================%
\begin{frame}
\Large
\textbf{Accuracy}\\
\begin{itemize}
\item Because most transactions are legitimate, accuracy is
not an appropriate metric for this problem.
\item  A classifier that always predicts that
transactions are legitimate could have a high accuracy score, but would not be
useful. 
\item For these reasons, classifiers are often evaluated using two additional
measures called \textbf{precision} and \textbf{recall}.
\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{Precision and Recall}
\begin{itemize}
\item \textbf{Precision} (or positive predictive value (PPV))
\[\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP}) \]

\item \textbf{Recall} (or Sensitivity , true positive rate (TPR))
% eqv. with hit rate, recall
\[ \mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})  \]

\bigskip
\item \textbf{Specificity} (or true negative rate (TNR))
\[\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{TN}+\mathit{FP}) \]
\end{itemize}


\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{ Some Related Metrics}
\begin{itemize}
\item Negative predictive value (NPV)
\[\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN}) \]
\item False positive rate (FPR) also called "fallout"
\[\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN}) \]
\item False negative rate (FNR)
\[\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) = 1 - \mathit{TPR} \]
\item False discovery rate (FDR)
\[\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} \]

\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{Precision and Recall}

\begin{itemize}
\item Precision is the
fraction of positive predictions that are correct. 
\item For instance, in our SMS spam
classifier, precision is the fraction of messages classified as spam that are actually
spam. 
\item Precision is given by the following ratio:
\end{itemize}

\[ P = \frac{TP}{TP + FP} \]
\end{frame}
%==================================%
\begin{frame}
	% from skikitlearnbook
	\frametitle{The Confusion Matrix}
	\begin{itemize}
		\item The confusion matrix indicates that there were four true negative predictions, three
		true positive predictions, two false negative predictions, and one false positive
		prediction. 
		\item Confusion matrices become more useful in multi-class problems, in
		which it can be difficult to determine the most frequent types of errors.
	\end{itemize}
	
\end{frame}
%=================================%
\begin{frame}
	\textbf{Recall}
\begin{itemize}
\item   Recall is the fraction of the truly
positive instances that the classifier recognizes. A recall score of one indicates
that the classifier did not make any false negative predictions. 

\item For our SMS spam
classifier, recall is the fraction of spam messages that were truly classified as spam.
\item Recall is calculated with the following ratio:
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.37\linewidth]{images/recall}

\end{figure}
\begin{itemize}
\item Recall is often called sensitivity in medical contexts.
\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\frametitle{Precision and Recall}
	\Large
\begin{itemize}
\item Individually, precision and recall are seldom informative; they are both incomplete
views of a classifier's performance. 
\item Both precision and recall can fail to distinguish
classifiers that perform well from certain types of classifiers that perform poorly.
\item A
trivial classifier could easily achieve a perfect recall score by predicting positive for
every instance.
\end{itemize}
 
\end{frame}
%=================================%
\begin{frame}
	\Large
\frametitle{Precision and Recall}
\begin{itemize}
\item For example, assume that a test set contains ten positive examples
and ten negative examples.
\item A classifier that predicts positive for every example will
achieve a recall of one, as follows:
\end{itemize}

10 1
10 0
R = =
+
\end{frame}
%=================================%
\begin{frame}
	
	\begin{itemize}
\item A classifier that predicts negative for every example, or that makes only false positive
and true negative predictions, will achieve a recall score of zero. 
\item Similarly, a classifier
that predicts that only a single instance is positive and happens to be correct will
achieve perfect precision.
	\end{itemize}

\end{frame}
%=================================%
\end{document}
