\documentclass[PredictiveAnalytics101.tex]{subfiles} 
\begin{document} 
% PLAN FOR SECTION
% ------------------------------------
% Confusion Matrix
% Accuracy Precision Recall
% F-measures
% Cost of MisClassification
% True Positive Rate
% Specificity and Sensitivitiy

% Re-writing a section from a shitty PACKT book
%============================%
\begin{frame}
\frametitle{Confusion Matrix}

	\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{confusionmatrix2}
\end{figure}

\end{frame}

%==================================%
\begin{frame}
\frametitle{Accuracy}
\textbf{Accuracy}\\
Accuracy measures a fraction of the classifier's predictions that are correct.
{\LARGE
\[ \mbox{Accuracy} =  \frac{TN + TP }{TN+FN+TP+FP}  \]
}
\end{frame}
%========================================== %
\begin{frame}
\Large
\frametitle{Accuracy}
\begin{center}	
	\begin{tabular}{|c|c|c|}
		\hline  &  Predict P & Predict N  \\ 
		\hline True P  & 100 & 70  \\ 
		\hline True N & 30 & 9800 \\ 
		\hline 
	\end{tabular} 
\end{center}
\bigskip
{\LARGE
	\[ \mbox{Accuracy} =  \frac{9900 }{10,000} = 0.99  \]
}
\end{frame}
%==================================%
\begin{frame}
\Large
	\textbf{Accuracy}\\
\begin{itemize}
\item However, accuracy is not an informative metric if the proportions of
the classes are skewed in the population. (\textbf{Class Imbalance})
\item For example, a classifier that predicts
whether or not credit card transactions are fraudulent may be more sensitive to
false negatives than to false positives. 
\item To promote customer satisfaction, the credit
card company may prefer to risk verifying legitimate transactions than risk ignoring
a fraudulent transaction. 
\end{itemize}

\end{frame}
%==================================%
\begin{frame}
\Large
\textbf{Accuracy}\\
\begin{itemize}
\item Because most transactions are legitimate, accuracy is
not an appropriate metric for this problem.
\item  A classifier that always predicts that
transactions are legitimate could have a high accuracy score, but would not be
useful. 
\item For these reasons, classifiers are often evaluated using two additional
measures called \textbf{precision} and \textbf{recall}.
\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{Precision and Recall}
\begin{itemize}
\item \textbf{Precision} (or positive predictive value (PPV))
\[\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP}) \]

\item \textbf{Recall} (or Sensitivity , true positive rate (TPR))
% eqv. with hit rate, recall
\[ \mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})  \]

\bigskip
\item \textbf{Specificity} (or true negative rate (TNR))
\[\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{TN}+\mathit{FP}) \]
\end{itemize}


\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{ Some Related Metrics}
\begin{itemize}
\item Negative predictive value (NPV)
\[\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN}) \]
\item False positive rate (FPR) also called "fallout"
\[\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN}) \]
\item False negative rate (FNR)
\[\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) = 1 - \mathit{TPR} \]
\item False discovery rate (FDR)
\[\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} \]

\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\Large
\textbf{Precision and Recall}

\begin{itemize}
\item Precision is the
fraction of positive predictions that are correct. 
\item For instance, in our SMS spam
classifier, precision is the fraction of messages classified as spam that are actually
spam. 
\item Precision is given by the following ratio:
\end{itemize}

\[ P = \frac{TP}{TP + FP} \]
\end{frame}
%==================================%
\begin{frame}
	% from skikitlearnbook
	\frametitle{The Confusion Matrix}
	\begin{itemize}
		\item The confusion matrix indicates that there were four true negative predictions, three
		true positive predictions, two false negative predictions, and one false positive
		prediction. 
		\item Confusion matrices become more useful in multi-class problems, in
		which it can be difficult to determine the most frequent types of errors.
	\end{itemize}
	
\end{frame}
%=================================%
\begin{frame}
	\textbf{Recall}
\begin{itemize}
\item   Recall is the fraction of the truly
positive instances that the classifier recognizes. A recall score of one indicates
that the classifier did not make any false negative predictions. 

\item For our SMS spam
classifier, recall is the fraction of spam messages that were truly classified as spam.
\item Recall is calculated with the following ratio:
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{recall}

\end{figure}
\begin{itemize}
\item Recall is often called sensitivity in medical contexts.
\end{itemize}

\end{frame}
%=================================%
\begin{frame}
\frametitle{Precision and Recall}
	\Large
\begin{itemize}
\item Individually, precision and recall are seldom informative; they are both incomplete
views of a classifier's performance. 
\item Both precision and recall can fail to distinguish
classifiers that perform well from certain types of classifiers that perform poorly.
\item A
trivial classifier could easily achieve a perfect recall score by predicting positive for
every instance.
\end{itemize}
 
\end{frame}
%=================================%
\begin{frame}
	\Large
\frametitle{Precision and Recall}
\begin{itemize}
\item For example, assume that a test set contains ten positive examples
and ten negative examples.
\item A classifier that predicts positive for every example will
achieve a recall of one, as follows:
\end{itemize}

10 1
10 0
R = =
+
\end{frame}
%=================================%
\begin{frame}
	
	\begin{itemize}
\item A classifier that predicts negative for every example, or that makes only false positive
and true negative predictions, will achieve a recall score of zero. 
\item Similarly, a classifier
that predicts that only a single instance is positive and happens to be correct will
achieve perfect precision.
	\end{itemize}

\end{frame}
%=================================%
\end{document}
