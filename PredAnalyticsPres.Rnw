\documentclass{beamer}

\usepackage{subfiles}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}


\begin{document}
%==============================================%
\subfile{00-leadoutslides.tex}
% A Few community announcements
%  - Big Data Spain
%  - EARL
%  - CSO Jobs
%  - PyCon Ireland
%==============================================%
% \subfile{01-Introduction.tex}
%  Discussion of Predictive Analytics
%  Machine Learning


%=================================================================================%

% \subfile{01-ModelBuilding.tex}

% 7 Step Process


%==============================================%
% \subfile{01-learningtypes.tex}

% Supervised and Unsupervised Learning
% What is the difference?
%==============================================%
% \subfile{01-problemtypes.tex}

% Classification and Regression
% Binary Classification Problems
% Logistic Regression

%==============================================%

% What is Distance Theory
% - Euclidean Distance
% - Mahalanobis Distance
% - Nearest Neighbour

%==============================================%

%\subfile{02-clustering.tex}
% Quick Discussion of Clustering Analysis
% Customer Segmentation

%==============================================%
% \subfile{02-ModelAppraisal-part1.tex}

% Confusion Matrix
% Accuracy Precision Recall
% F-measures
% Cost of MisClassification
% True Positive Rate
% Specificity and Sensitivitiy


%==============================================%
% \subfile{02-ModelAppraisal-part3.tex}
%==============================================%
% \subfile{02-ModelAppraisal-part2.tex}

% Graphical Methods
% ROC Curves
% Lift
%==============================================%
% \subfile{03-LawOfParsimony.tex}
%\subfile{03-ImprovingModels.tex}


% Overfitting
% Testing and Training
% k-folds Cross Validation

%===============================================================================%


% - Dimensionality Reduction

% - Feature Selection / Variable Selection


%==============================================%
\begin{frame}
  \frametitle{Introduction}
  \begin{enumerate}
  \item Get Data
  \item Clean Data
  \item Model Data
  \item Make Predictions
  \item Test on More Data
  \item Goto 1
  \end{enumerate}
\end{frame}
\begin{frame}
  \frametitle{Why should you care?}
  \begin{itemize}
  \item All models are wrong, but some are useful
  \item If you can predict an outcome of interest, you can make money :)
  \item Or get published in peer-reviewed journals!
  \end{itemize}
\end{frame}
\begin{frame}
\frametitle{Steps}
\begin{enumerate}
\item Defining the Problem
\item Collecting the Data
\item Processing The Data
\item Run an initial model
\item Evaluate the initial model
\item Select a final model
\item Testing the Model
\item Use the Model
\end{enumerate}
\end{frame}
\begin{frame}
  \frametitle{Defining the Problem}
  \begin{itemize}
  \item Entire books have been written about this
  \item Sample Problem: Optimism appears to be negatively correlated to health 
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Collecting the Data}
  \begin{itemize}
  \item Surveys
  \item Experiments
  \item Web Scraping
  \item Observational Data (logs etc)
  \item Standard Data
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Sample Problem Data}
  \begin{itemize}
  \item Two samples (N=392, N=1101)
  \item All survey data (optimism, health and other health-related variables)
  \item Problem: Optimism is always positively correlated with Health, but this is not true in the sample
  \end{itemize}
\end{frame}
<<options, echo=FALSE, results='hide'>>=
knitr::opts_chunk$set(echo=TRUE, cache=FALSE)
options(width=50)
@ 
\begin{frame}
  \frametitle{Problem Graph}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<data, results='hide'>>=
hom <- read.csv("homfinal.csv") 
source("recode.R")
system("wc -l recode.R")
@         
    \end{column}
    \begin{column}{0.5\textwidth}
<<optplot, echo=FALSE, warning=FALSE>>=
suppressMessages(require(ggplot2))
ggplot(hom1, aes(x=optimism, y=generalhealth))+geom_point()+geom_smooth(method="lm")
@       
    \end{column}
  \end{columns}

\end{frame}
\begin{frame}
  \frametitle{Processing the Data}
  \begin{itemize}
  \item Most importantly, split your data
  \item You need at a minimum, three splits
  \item Training Data (approx 60\%)
  \item Test Data (approx 20\%)
  \item Validation Data (approx 20\%)
  \item If you don't have this, you will fail to predict accurately
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Splitting Data}
<<removefakedata, echo=FALSE, results="hide">>=
hom1 <- hom1[,grep("^[A-Z][a-z]+", x=names(hom1), invert=TRUE)]
@   

<<splitdata, results='hide', tidy=TRUE>>=
set.seed(23)

suppressMessages(require(caret))

hom.full <- na.omit(hom1)

split <- with(hom.full, 
              createDataPartition(optimism, times=1, p=0.6, list=FALSE))

hom.train <- hom.full[split,]

hom.rest <- hom.full[-split,]

split2 <- with(hom.rest, 
               createDataPartition(optimism, times=1, p=0.5, list=FALSE))

hom.test <- hom.rest[split2,]

hom.validation <- hom.rest[-split2,]

@       


\end{frame}
\begin{frame}
  \frametitle{Cleaning the Data}
  \begin{itemize}
  \item All data is wrong
    
  \item You need to make sure that the data is not wrong in as many obvious ways as possible
    
  \item Graphs, summaries et al are great for this
    
  \item Additionally, you may need different representations of data for different problems
  \end{itemize}
\end{frame}
<<packages, echo=FALSE, results='hide'>>=
suppressMessages(require(psych))
suppressMessages(require(xtable))
suppressMessages(require(reshape2))
@ 
\begin{frame}
  \frametitle{Examples}
<<pairsplot, echo=FALSE, out.height="0.8\\textheight">>=
totals <- hom.train[,60:69]
pairs.panels(totals)
@       

\end{frame}
\begin{frame}
  \frametitle{Scale Problems}
<<totals, echo=FALSE, out.height="0.8\\textheight">>=
totals.m <- suppressMessages(melt(totals))
ggplot(totals.m, aes(x=variable, y=value))+geom_boxplot()+theme(axis.text.x=element_text(angle=45))

@   
\end{frame}
\begin{frame}
  \frametitle{Learnings}
  \begin{itemize}
  \item The items are on wildly different scales
    
  \item The values are numerical, but from a fixed set
    
  \item Responses to questions on a number of difficult scales
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Setting Up Controls}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item Set up the way in which we'll train our models
        
      \item We could also specify the number of different parameters to estimate
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
<<controls, tidy=TRUE>>=
suppressMessages(require(doMC))
registerDoMC(cores=3)
ctrl = trainControl(method="repeatedcv", number=10, repeats=5, selectionFunction = "oneSE")
tl <- 8
Metric <- "Rsquared"
moreArgs <- c(fitBest=FALSE, returnData=TRUE)
@       
    \end{column}
    
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Modelling!}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item Really, cleaning the data takes a lot longer and is a part of the process throughout the analysis
        
      \item We'll skip that though, cos models are much more interesting
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
<<caretcode, tidy=TRUE,  results="hide">>=
hom.lm.train <- suppressWarnings(train(optimism~., method="lm", data=hom.train, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE))
train.totals <- dplyr::select(hom.train, 60:69)
hom.lm.train.totals <- train(optimism~., method="lm", data=train.totals, preProcess="scale", tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)

@       

    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Model Predictions}
  \begin{columns}
    \begin{column}{0.4\textwidth}
<<outputlm, echo=FALSE, results="asis">>=
print(xtable(summary(hom.lm.train.totals)), scalebox=0.5)
@       
    \end{column}
    \begin{column}{0.6\textwidth}
<<varImpPlot, echo=FALSE>>=
plot(varImp(hom.lm.train.totals))
@       
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{This Model Sucks}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item The answer to this problem is clearly to fit more models
        
      \item Lets try regularised regression and random forest
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
<<moremodels, format="markup",  results="hide">>=
hom.rf.train.totals <- suppresWarnings(suppressMessages(train(optimism~., method="rf", data=train.totals, preProcess="scale", tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)))
hom.glmnet.train.totals <- suppressWarnings(suppressMessages(train(optimism~., method="glmnet", data=train.totals, preProcess="scale", tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)))
@       
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Do these Models Suck?}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<varimpRF, echo=FALSE>>=
plot(varImp(hom.glmnet.train.totals), main="Glmnet variable importance")

@       
    \end{column}
    \begin{column}{0.5\textwidth}
<<varimprf, echo=FALSE>>=
varImpPlot(hom.rf.train.totals$finalModel, main="Random Forest Variable Importance")
@       
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Yet More Models}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<glmboost, echo=FALSE>>=
hom.gam.train.totals <- suppressWarnings(suppressMessages(train(optimism~., method="gam", data=train.totals, preProcess="scale", tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)))
hom.pls.train.totals <- suppressMessages(train(optimism~., method="pls", data=train.totals, preProcess=c("scale", "center"),  tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE))
@       
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item These ones suck too :(
        
      \item So we've tried standard linear models, some non-linear models and some penalised models
        
      \item Even with the training set, we struggle to achieve greater than .3\% R-Sq (which is pretty awful in and of itself)
        
      \item What next? 
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Redefining the Problem}
  \begin{itemize}
  \item We're currently using the totals as our training data
    
  \item This is typical in academic (psychological) research
    
  \item However, it essentially means that we are throwing away the majority of the information in the data
    
  \item So, in order to improve predictive accuracy, lets start using as much of the data as possible
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{New Features}
  \begin{columns}
    
    \begin{column}{0.5\textwidth}
<<samemodelsdifferentdata, eval=2>>=
hom.glmnet.train <- suppressWarnings(train(optimism~., method="glmnet", data=hom.train, trControl=ctrl, metric=Metric, preProcess=c("scale", "center"), fitBest=FALSE, returnData=TRUE))
hom.rf.train <- suppressMessages(suppressWarnings(train(optimism~., method="rf", data=hom.train, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)))
@       
    \end{column}
    \begin{column}{0.5\textwidth}
<<actuallyprint, echo=FALSE, results="hide">>=
rf.pr <- as.data.frame(print(hom.rf.train))
rownames(rf.pr) <- 1:nrow(rf.pr)
rf.pr2 <- dplyr::select(rf.pr, mtry, RMSE, Rsquared)
@       
<<printoutput, echo=FALSE, results="asis">>=
print(xtable(rf.pr2), scalebox=0.5, include.rownames=FALSE)
@       
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{What Actually Happened}
  \begin{itemize}
  \item So, remember we took all of the questions as features
    
  \item However, the totals are averages of the questions
    
  \item Therefore, we can predict the averages perfectly with these features
    
  \item They are somewhat unlikely to generalise :)
    
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{More Modelling!}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<lotrhide, echo=FALSE, results="hide">>=
hom.train.opt <- dplyr::select(hom.train, grep("^LOTRQ", x=names(hom.train),invert=TRUE))
hom.test.opt <- dplyr::select(hom.test, grep("^LOTRQ", x=names(hom.test),invert=TRUE))
hom.val.opt <- dplyr::select(hom.validation, grep("^LOTRQ", x=names(hom.validation),invert=TRUE))
@       
<<removelotr, echo=TRUE, tidy=TRUE>>=
hom.glmnet.train.opt <- suppressWarnings(train(optimism~., method="glmnet", data=hom.train.opt, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE))
hom.rf.train.opt <- suppressWarnings(train(optimism~., method="rf", data=hom.train.opt, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE))
hom.gam.train.opt <- train(optimism~., method="gam", data=hom.train.opt,  tuneLength=, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)
hom.pls.train.opt <- train(optimism~., method="pls", data=hom.train.opt, preProcess="scale", tuneLength=tl, trControl=ctrl, metric=Metric, fitBest=FALSE, returnData=TRUE)
@             
    \end{column}
    \begin{column}{0.5\textwidth}
<<dataprint, echo=FALSE, results="hide">>=
rf.opt.pr <- as.data.frame(print(hom.rf.train.opt))
rf.opt.pr2 <- dplyr::select(rf.pr, mtry, RMSE, Rsquared)
@       
<<dataoutput, echo=FALSE, results="asis">>=
print(xtable(rf.opt.pr2), scalebox=0.5, include.rownames=FALSE)
@ 
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Make some Predictions}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<predictions, echo=TRUE>>=
glmnet.pred.obs <- data.frame(pred=predict(hom.glmnet.train.opt, newdata=hom.test.opt), obs=hom.test.opt[["optimism"]], Model="glmnet")
rf.pred.obs <- data.frame(pred=predict(hom.rf.train.opt, newdata=hom.test.opt), obs=hom.test.opt$optimism, Model="rf")
gam.pred.obs <- data.frame(pred=predict(hom.gam.train.opt, newdata=hom.test.opt), obs=hom.test.opt[["optimism"]], Model="gam")
pls.pred.obs <- data.frame(pred=predict(hom.pls.train.opt, newdata=hom.test.opt), obs=hom.test.opt[["optimism"]], Model="pls")
pred.obs.all <- rbind(glmnet.pred.obs, rf.pred.obs, gam.pred.obs, pls.pred.obs)
@       
<<pstresample, echo=FALSE>>=
resampFunc <- function(data)  {
    ans <- with(data, postResample(pred, obs))
    
}
glmnet.acc <- resampFunc(glmnet.pred.obs)
pls.acc <- resampFunc(pls.pred.obs)
gam.acc <- resampFunc(gam.pred.obs)
rf.acc <- resampFunc(rf.pred.obs)
acc.all <- as.data.frame(rbind(glmnet.acc, pls.acc, gam.acc, rf.acc))

@ 
\begin{itemize}
\item So these models are better, but they still mostly suck
\end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
<<accrmse, echo=FALSE, results="asis">>=
xtable(acc.all)
@ 
    \end{column}
  \end{columns}
\end{frame}
\begin{frame}
  \frametitle{Predictions Vs Observed}
<<predvsobs>>=
ggplot(pred.obs.all, aes(x=pred, y=obs))+geom_point()+geom_smooth(method="lm")+facet_grid(.~Model)
@   
\end{frame}
\begin{frame}
  \frametitle{These models still suck}
  
\end{frame}
\end{document}
%- http://www.avanade.com/~/media/documents/bi-white-paper-healthcare-analytics-practical-predictive-analytics-101-may-2013.pdf






%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
